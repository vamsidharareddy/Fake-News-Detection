{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive - applicable, if working on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G-evkk1vIX5",
        "outputId": "de3ab5f4-d753-4c29-e85c-2213b511e824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search cv"
      ],
      "metadata": {
        "id": "iW8hgx7Auw_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from joblib import dump\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/D2/training.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/D2/validation.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D2/testmod.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_data['statement']\n",
        "y_train = train_data['label']\n",
        "X_val = val_data['statement']\n",
        "y_val = val_data['label']\n",
        "X_test = test_data['statement']\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1000)),\n",
        "    ('rf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200],\n",
        "    'rf__max_depth': [None, 10, 20],\n",
        "    'rf__min_samples_split': [2, 5],\n",
        "    'rf__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Save the best model\n",
        "dump(grid_search.best_estimator_, '/content/drive/MyDrive/D2/D2_Random/grid_rf_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz4GUf_Fv0L3",
        "outputId": "f57f49bf-da7e-4739-aabb-9e8fe271059e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/D2/D2_Random/grid_rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayes Optimization"
      ],
      "metadata": {
        "id": "COznRprKuydI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRLZK98yvioh",
        "outputId": "be5c960c-658f-474c-a99c-cc5854c3531a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.4.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer\n",
        "from joblib import dump\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/D2/training.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/D2/validation.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D2/testmod.csv')\n",
        "\n",
        "# Assume 'label' is the target and other columns are features\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Combine training and validation data\n",
        "X_train_combined = pd.concat([X_train, X_val])\n",
        "y_train_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = X_train_combined.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train_combined.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Handle unknown categories during testing\n",
        "    ])\n",
        "\n",
        "# Define the Random Forest model and hyperparameter space\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "param_space = {\n",
        "    'classifier__n_estimators': Integer(100, 300),\n",
        "    'classifier__max_depth': Integer(10, 30),\n",
        "    'classifier__min_samples_split': Integer(2, 10),\n",
        "    'classifier__min_samples_leaf': Integer(1, 4)\n",
        "}\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', rf_model)])\n",
        "\n",
        "# Initialize Bayesian Optimization\n",
        "bayes_search = BayesSearchCV(estimator=pipeline, search_spaces=param_space, n_iter=30, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "bayes_search.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Save the best model\n",
        "model_save_path = '/content/drive/MyDrive/D2/D2_Random/bayes_rf_model.pkl'\n",
        "dump(bayes_search.best_estimator_, model_save_path)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = bayes_search.best_estimator_.predict(X_test) # No need to replace unknown categories now\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeo-aOZ8v2Lh",
        "outputId": "887b8e73-407e-4b95-dba7-edcff7d183ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Test Accuracy: 0.43646408839779005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PBT"
      ],
      "metadata": {
        "id": "WrEElz43uzEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[tune]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDrpl3UvvjBq",
        "outputId": "bbb62bd2-6882-4bed-e9e0-b06f88b71625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.32.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.0.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from joblib import dump\n",
        "import ray\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "# Load datasets (replace paths with your actual paths)\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/D2/training.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/D2/validation.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D2/testmod.csv')\n",
        "\n",
        "# Assume 'label' is the target and other columns are features\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Fit and transform on combined training and validation data\n",
        "X_train_combined = pd.concat([X_train, X_val])\n",
        "X_train_combined_processed = preprocessor.fit_transform(X_train_combined)\n",
        "\n",
        "# Transform test data\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Put large data objects into Ray object store\n",
        "X_train_combined_ref = ray.put(X_train_combined_processed)\n",
        "y_train_combined_ref = ray.put(pd.concat([y_train, y_val]))\n",
        "\n",
        "# Define the parameter search space\n",
        "param_space = {\n",
        "    'n_estimators': tune.randint(100, 300),\n",
        "    'max_depth': tune.randint(10, 30),\n",
        "    'min_samples_split': tune.randint(2, 10),\n",
        "    'min_samples_leaf': tune.randint(1, 4)\n",
        "}\n",
        "\n",
        "# Define the scheduler\n",
        "pbt_scheduler = PopulationBasedTraining(\n",
        "    time_attr=\"training_iteration\",\n",
        "    metric=\"mean_accuracy\",\n",
        "    mode=\"max\",\n",
        "    perturbation_interval=5,\n",
        "    hyperparam_mutations={\n",
        "        \"n_estimators\": lambda: np.random.randint(100, 300),\n",
        "        \"max_depth\": lambda: np.random.randint(10, 30),\n",
        "        \"min_samples_split\": lambda: np.random.randint(2, 10),\n",
        "        \"min_samples_leaf\": lambda: np.random.randint(1, 4),\n",
        "    })\n",
        "\n",
        "# Define the objective function\n",
        "def train_model(config):\n",
        "    # Get the data references from the object store\n",
        "    X_train_combined_processed = ray.get(X_train_combined_ref)\n",
        "    y_train_combined = ray.get(y_train_combined_ref)\n",
        "\n",
        "    # Define the model pipeline with current config\n",
        "    rf_model = RandomForestClassifier(**config, random_state=42)\n",
        "\n",
        "    # Fit the model\n",
        "    rf_model.fit(X_train_combined_processed, y_train_combined)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred = rf_model.predict(preprocessor.transform(X_val))\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "    return {\"mean_accuracy\": accuracy}\n",
        "\n",
        "# Perform Population Based Training\n",
        "analysis = tune.run(\n",
        "    train_model,\n",
        "    config=param_space,\n",
        "    scheduler=pbt_scheduler,\n",
        "    stop={\"training_iteration\": 5},\n",
        "    num_samples=10,\n",
        "    resources_per_trial={\"cpu\": 2, \"gpu\": 0.5}  # Adjust based on your available resources\n",
        ")\n",
        "\n",
        "# Get the best performing model\n",
        "best_trial = analysis.get_best_trial(\"mean_accuracy\", \"max\", \"last\")\n",
        "best_config = best_trial.config\n",
        "\n",
        "# Define the best model\n",
        "best_model = RandomForestClassifier(**best_config, random_state=42)\n",
        "\n",
        "# Fit the best model on the combined training and validation set\n",
        "best_model.fit(X_train_combined_processed, pd.concat([y_train, y_val]))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_processed)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the best model\n",
        "model_save_path = '/content/drive/MyDrive/D2/D2_Random/pbt_rf_model.pkl'\n",
        "dump(best_model, model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMNAik_C29rR",
        "outputId": "ef349a77-8b10-4c4f-c0e6-77bb33c6b71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-18 07:18:18,330\tINFO worker.py:1621 -- Calling ray.init() again after it has already been called.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py:730: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_model_2024-07-18_07-18-18   |\n",
            "+--------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator             |\n",
            "| Scheduler                        PopulationBasedTraining           |\n",
            "| Number of trials                 10                                |\n",
            "+--------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_model_2024-07-18_07-18-18\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-07-18_07-11-13_646063_16086/artifacts/2024-07-18_07-18-18/train_model_2024-07-18_07-18-18/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2024-07-18 07:18:19. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       n_estimators     max_depth     min_samples_split     min_samples_leaf |\n",
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "| train_model_e8294_00000   PENDING               275            13                     7                    2 |\n",
            "| train_model_e8294_00001   PENDING               143            11                     9                    3 |\n",
            "| train_model_e8294_00002   PENDING               144            26                     5                    1 |\n",
            "| train_model_e8294_00003   PENDING               132            21                     9                    3 |\n",
            "| train_model_e8294_00004   PENDING               194            22                     2                    1 |\n",
            "| train_model_e8294_00005   PENDING               294            23                     4                    1 |\n",
            "| train_model_e8294_00006   PENDING               260            18                     5                    2 |\n",
            "| train_model_e8294_00007   PENDING               168            29                     2                    1 |\n",
            "| train_model_e8294_00008   PENDING               261            23                     5                    1 |\n",
            "| train_model_e8294_00009   PENDING               159            29                     3                    1 |\n",
            "+--------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00000 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00000 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 13 |\n",
            "| min_samples_leaf                           2 |\n",
            "| min_samples_split                          7 |\n",
            "| n_estimators                             275 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00000 finished iteration 1 at 2024-07-18 07:18:26. Total running time: 7s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00000 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                         0.47675 |\n",
            "| time_total_s                             0.47675 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                             0.5466 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00000 completed after 1 iterations at 2024-07-18 07:18:26. Total running time: 7s\n",
            "\n",
            "Trial train_model_e8294_00001 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00001 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 11 |\n",
            "| min_samples_leaf                           3 |\n",
            "| min_samples_split                          9 |\n",
            "| n_estimators                             143 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00001 finished iteration 1 at 2024-07-18 07:18:29. Total running time: 11s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00001 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                         0.28704 |\n",
            "| time_total_s                             0.28704 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                             0.5466 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00001 completed after 1 iterations at 2024-07-18 07:18:29. Total running time: 11s\n",
            "\n",
            "Trial train_model_e8294_00002 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00002 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 26 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          5 |\n",
            "| n_estimators                             144 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00002 finished iteration 1 at 2024-07-18 07:18:32. Total running time: 14s\n",
            "+-------------------------------------------------+\n",
            "| Trial train_model_e8294_00002 result            |\n",
            "+-------------------------------------------------+\n",
            "| checkpoint_dir_name                             |\n",
            "| time_this_iter_s                         0.4158 |\n",
            "| time_total_s                             0.4158 |\n",
            "| training_iteration                            1 |\n",
            "| mean_accuracy                            0.5466 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00002 completed after 1 iterations at 2024-07-18 07:18:32. Total running time: 14s\n",
            "\n",
            "Trial train_model_e8294_00003 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00003 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 21 |\n",
            "| min_samples_leaf                           3 |\n",
            "| min_samples_split                          9 |\n",
            "| n_estimators                             132 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00003 finished iteration 1 at 2024-07-18 07:18:37. Total running time: 18s\n",
            "+-------------------------------------------------+\n",
            "| Trial train_model_e8294_00003 result            |\n",
            "+-------------------------------------------------+\n",
            "| checkpoint_dir_name                             |\n",
            "| time_this_iter_s                         0.3762 |\n",
            "| time_total_s                             0.3762 |\n",
            "| training_iteration                            1 |\n",
            "| mean_accuracy                            0.5466 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00003 completed after 1 iterations at 2024-07-18 07:18:37. Total running time: 18s\n",
            "\n",
            "Trial train_model_e8294_00004 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00004 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 22 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          2 |\n",
            "| n_estimators                             194 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00004 finished iteration 1 at 2024-07-18 07:18:41. Total running time: 23s\n",
            "+-------------------------------------------------+\n",
            "| Trial train_model_e8294_00004 result            |\n",
            "+-------------------------------------------------+\n",
            "| checkpoint_dir_name                             |\n",
            "| time_this_iter_s                         0.5408 |\n",
            "| time_total_s                             0.5408 |\n",
            "| training_iteration                            1 |\n",
            "| mean_accuracy                            0.5466 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00004 completed after 1 iterations at 2024-07-18 07:18:41. Total running time: 23s\n",
            "\n",
            "Trial train_model_e8294_00005 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00005 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 23 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          4 |\n",
            "| n_estimators                             294 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00005 finished iteration 1 at 2024-07-18 07:18:45. Total running time: 26s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00005 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                         0.78671 |\n",
            "| time_total_s                             0.78671 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                             0.5466 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00005 completed after 1 iterations at 2024-07-18 07:18:45. Total running time: 26s\n",
            "\n",
            "Trial train_model_e8294_00006 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00006 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 18 |\n",
            "| min_samples_leaf                           2 |\n",
            "| min_samples_split                          5 |\n",
            "| n_estimators                             260 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00006 finished iteration 1 at 2024-07-18 07:18:48. Total running time: 30s\n",
            "+-------------------------------------------------+\n",
            "| Trial train_model_e8294_00006 result            |\n",
            "+-------------------------------------------------+\n",
            "| checkpoint_dir_name                             |\n",
            "| time_this_iter_s                         0.4767 |\n",
            "| time_total_s                             0.4767 |\n",
            "| training_iteration                            1 |\n",
            "| mean_accuracy                            0.5466 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00006 completed after 1 iterations at 2024-07-18 07:18:48. Total running time: 30s\n",
            "\n",
            "Trial status: 7 TERMINATED | 3 PENDING\n",
            "Current time: 2024-07-18 07:18:49. Total running time: 30s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         n_estimators     max_depth     min_samples_split     min_samples_leaf        acc     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_model_e8294_00000   TERMINATED              275            13                     7                    2   0.546599        1           0.476752 |\n",
            "| train_model_e8294_00001   TERMINATED              143            11                     9                    3   0.546599        1           0.287035 |\n",
            "| train_model_e8294_00002   TERMINATED              144            26                     5                    1   0.546599        1           0.415797 |\n",
            "| train_model_e8294_00003   TERMINATED              132            21                     9                    3   0.546599        1           0.376202 |\n",
            "| train_model_e8294_00004   TERMINATED              194            22                     2                    1   0.546599        1           0.540803 |\n",
            "| train_model_e8294_00005   TERMINATED              294            23                     4                    1   0.546599        1           0.786707 |\n",
            "| train_model_e8294_00006   TERMINATED              260            18                     5                    2   0.546599        1           0.476699 |\n",
            "| train_model_e8294_00007   PENDING                 168            29                     2                    1                                        |\n",
            "| train_model_e8294_00008   PENDING                 261            23                     5                    1                                        |\n",
            "| train_model_e8294_00009   PENDING                 159            29                     3                    1                                        |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00007 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00007 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 29 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          2 |\n",
            "| n_estimators                             168 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00007 finished iteration 1 at 2024-07-18 07:18:54. Total running time: 36s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00007 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                          0.8825 |\n",
            "| time_total_s                              0.8825 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                            0.54723 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00007 completed after 1 iterations at 2024-07-18 07:18:54. Total running time: 36s\n",
            "\n",
            "Trial train_model_e8294_00008 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00008 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 23 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          5 |\n",
            "| n_estimators                             261 |\n",
            "+----------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00008 finished iteration 1 at 2024-07-18 07:19:00. Total running time: 41s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00008 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                         1.12117 |\n",
            "| time_total_s                             1.12117 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                             0.5466 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00008 completed after 1 iterations at 2024-07-18 07:19:00. Total running time: 41s\n",
            "\n",
            "Trial train_model_e8294_00009 started with configuration:\n",
            "+----------------------------------------------+\n",
            "| Trial train_model_e8294_00009 config         |\n",
            "+----------------------------------------------+\n",
            "| max_depth                                 29 |\n",
            "| min_samples_leaf                           1 |\n",
            "| min_samples_split                          3 |\n",
            "| n_estimators                             159 |\n",
            "+----------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-07-18 07:19:03,835\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_model_2024-07-18_07-18-18' in 0.0093s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_model_e8294_00009 finished iteration 1 at 2024-07-18 07:19:03. Total running time: 45s\n",
            "+--------------------------------------------------+\n",
            "| Trial train_model_e8294_00009 result             |\n",
            "+--------------------------------------------------+\n",
            "| checkpoint_dir_name                              |\n",
            "| time_this_iter_s                         0.46951 |\n",
            "| time_total_s                             0.46951 |\n",
            "| training_iteration                             1 |\n",
            "| mean_accuracy                            0.54723 |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "Trial train_model_e8294_00009 completed after 1 iterations at 2024-07-18 07:19:03. Total running time: 45s\n",
            "\n",
            "Trial status: 10 TERMINATED\n",
            "Current time: 2024-07-18 07:19:03. Total running time: 45s\n",
            "Logical resource usage: 2.0/2 CPUs, 0.5/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         n_estimators     max_depth     min_samples_split     min_samples_leaf        acc     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_model_e8294_00000   TERMINATED              275            13                     7                    2   0.546599        1           0.476752 |\n",
            "| train_model_e8294_00001   TERMINATED              143            11                     9                    3   0.546599        1           0.287035 |\n",
            "| train_model_e8294_00002   TERMINATED              144            26                     5                    1   0.546599        1           0.415797 |\n",
            "| train_model_e8294_00003   TERMINATED              132            21                     9                    3   0.546599        1           0.376202 |\n",
            "| train_model_e8294_00004   TERMINATED              194            22                     2                    1   0.546599        1           0.540803 |\n",
            "| train_model_e8294_00005   TERMINATED              294            23                     4                    1   0.546599        1           0.786707 |\n",
            "| train_model_e8294_00006   TERMINATED              260            18                     5                    2   0.546599        1           0.476699 |\n",
            "| train_model_e8294_00007   TERMINATED              168            29                     2                    1   0.547229        1           0.882502 |\n",
            "| train_model_e8294_00008   TERMINATED              261            23                     5                    1   0.546599        1           1.12117  |\n",
            "| train_model_e8294_00009   TERMINATED              159            29                     3                    1   0.547229        1           0.469511 |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Test Accuracy: 0.43567482241515393\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/D2/D2_Random/pbt_rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genetic"
      ],
      "metadata": {
        "id": "Ha2FpwICuyk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EJZB1mv22-iR",
        "outputId": "7c1bfa9e-10aa-4a62-82ad-5bf1ad817f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5oDEudLz2-ms",
        "outputId": "a4e23cf9-7724-46be-b636-1fee54609d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tpot\n",
            "  Downloading TPOT-0.12.2-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.11.4)\n",
            "Collecting scikit-learn>=1.4.1 (from tpot)\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: deap>=1.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.1)\n",
            "Collecting update-checker>=0.16 (from tpot)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (4.66.4)\n",
            "Collecting stopit>=1.1.1 (from tpot)\n",
            "  Downloading stopit-1.1.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.2)\n",
            "Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.1->tpot) (3.5.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.16->tpot) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2024.7.4)\n",
            "Building wheels for collected packages: stopit\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11938 sha256=5373426f193e79b52861d0d4da9cd257084cd7b3a1a46b24d92a45d97bb0b57d\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/f9/87/bf5b3d565c2a007b4dae9d8142dccc85a9f164e517062dd519\n",
            "Successfully built stopit\n",
            "Installing collected packages: stopit, update-checker, scikit-learn, tpot\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.5.1 stopit-1.1.2 tpot-0.12.2 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dHWudvT0Jrgh",
        "outputId": "2745e036-f680-4885-ec25-2640c5dea070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tpot import TPOTClassifier\n",
        "import joblib\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/D2/training.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/D2/validation.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D2/testmod.csv')\n",
        "\n",
        "# Assume 'label' is the target and other columns are features\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Combine training and validation data\n",
        "X_train_combined = pd.concat([X_train, X_val])\n",
        "y_train_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = X_train_combined.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train_combined.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with the preprocessor and a placeholder for the classifier\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])\n",
        "\n",
        "# Define the TPOTClassifier for Genetic Algorithm optimization\n",
        "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, random_state=42, config_dict={\n",
        "    'sklearn.ensemble.RandomForestClassifier': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "})\n",
        "\n",
        "# Fit the TPOT classifier\n",
        "tpot.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Get the best pipeline\n",
        "best_pipeline = tpot.fitted_pipeline_\n",
        "\n",
        "# Save the best model\n",
        "model_save_path = '/content/drive/MyDrive/D2/D2_Random/genetic_rf_model.pkl'\n",
        "joblib.dump(best_pipeline, model_save_path)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "PJYcmor12-wv",
        "outputId": "73db2e15-fa96-4167-ec79-4f415e44e037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1f08795e8d2>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtpot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtpot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTPOTRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/tpot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPOTBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassifier_config_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregressor_config_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tpot/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingIndicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_knn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_pandas_na\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.10/dist-packages/sklearn/base.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperband"
      ],
      "metadata": {
        "id": "CNcnWqqbuzcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperopt"
      ],
      "metadata": {
        "id": "-7D25Ph22_rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import HyperBandScheduler\n",
        "from ray.tune.sklearn import TuneSearchCV\n",
        "import joblib\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/D2/training.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/D2/validation.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D2/testmod.csv')\n",
        "\n",
        "# Assume 'label' is the target and other columns are features\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Combine training and validation data\n",
        "X_train_combined = pd.concat([X_train, X_val])\n",
        "y_train_combined = pd.concat([y_train, y_val])\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = X_train_combined.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train_combined.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define the parameter search space\n",
        "param_space = {\n",
        "    'classifier__n_estimators': tune.randint(100, 300),\n",
        "    'classifier__max_depth': tune.randint(10, 30),\n",
        "    'classifier__min_samples_split': tune.randint(2, 10),\n",
        "    'classifier__min_samples_leaf': tune.randint(1, 4)\n",
        "}\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Define the Hyperband scheduler\n",
        "scheduler = HyperBandScheduler(max_t=50, grace_period=1)\n",
        "\n",
        "# Initialize TuneSearchCV with Hyperband\n",
        "tune_search = TuneSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_space,\n",
        "    n_trials=30,\n",
        "    early_stopping=True,\n",
        "    max_iters=10,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    scheduler=scheduler\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "tune_search.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Save the best model\n",
        "model_save_path = '/content/drive/MyDrive/D2/D2_Random/hyperband_rf_model.pkl'\n",
        "joblib.dump(tune_search.best_estimator_, model_save_path)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = tune_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "npEC_WJg2_xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT + GSCV"
      ],
      "metadata": {
        "id": "4WXYF_6qpfwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import load\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model_path = '/content/drive/MyDrive/D2/bert_model/bert_model.pth'\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "bert_model.load_state_dict(torch.load(bert_model_path))\n",
        "\n",
        "# Ensure BERT model is in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the best Random Forest model\n",
        "rf_model = load('/content/drive/MyDrive/D2/D2_Random/grid_rf_model.pkl')\n",
        "\n",
        "# Function to get BERT predictions in batches\n",
        "def get_bert_predictions_batch(model, data, batch_size=32):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_data = data[i:i+batch_size].tolist()\n",
        "        inputs = tokenizer(batch_data, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "    return predictions\n",
        "\n",
        "# Get BERT predictions for the test data\n",
        "bert_predictions = get_bert_predictions_batch(bert_model, test_data['statement'])\n",
        "\n",
        "# Get Random Forest predictions\n",
        "X_test_tfidf = rf_model.named_steps['tfidf'].transform(X_test)\n",
        "rf_predictions = rf_model.named_steps['rf'].predict(X_test_tfidf)\n",
        "\n",
        "# Combine predictions (simple majority voting)\n",
        "final_predictions = (bert_predictions + rf_predictions) / 2\n",
        "final_predictions = (final_predictions > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f'Ensemble Model Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gbvOtOBtDpz",
        "outputId": "44b4e2f1-dd3d-40eb-a862-7673721805a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Accuracy: 0.3898973954222573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with sampling"
      ],
      "metadata": {
        "id": "jDXBN0GNu29I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import load\n",
        "import numpy as np\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model_path = '/content/drive/MyDrive/D2/bert_model/bert_model.pth'\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "bert_model.load_state_dict(torch.load(bert_model_path))\n",
        "\n",
        "# Ensure BERT model is in evaluation mode\n",
        "bert_model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the best Random Forest model\n",
        "rf_model = load('/content/drive/MyDrive/D2/D2_Random/grid_rf_model.pkl')\n",
        "\n",
        "# Function to get BERT predictions in batches with subset sampling\n",
        "def get_bert_predictions_batch(model, data, batch_size=32, subset_size=0.1):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    num_samples = int(len(data) * subset_size)\n",
        "    indices = np.random.choice(len(data), num_samples, replace=False)\n",
        "    sampled_data = data.iloc[indices]\n",
        "    for i in range(0, len(sampled_data), batch_size):\n",
        "        batch_data = sampled_data[i:i+batch_size].tolist()\n",
        "        inputs = tokenizer(batch_data, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.extend(batch_predictions)\n",
        "    return np.array(predictions), indices\n",
        "\n",
        "# Get BERT predictions for the test data\n",
        "bert_predictions, sampled_indices = get_bert_predictions_batch(bert_model, test_data['statement'])\n",
        "\n",
        "# Get Random Forest predictions for the same subset\n",
        "X_test_sampled = rf_model.named_steps['tfidf'].transform(X_test.iloc[sampled_indices])\n",
        "rf_predictions = rf_model.named_steps['rf'].predict(X_test_sampled)\n",
        "\n",
        "# Combine predictions (simple majority voting)\n",
        "final_predictions = (bert_predictions + rf_predictions) / 2\n",
        "final_predictions = (final_predictions > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "accuracy = accuracy_score(y_test.iloc[sampled_indices], final_predictions)\n",
        "print(f'Ensemble Model Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYSOJEaEu7aQ",
        "outputId": "cdd41592-f8a5-4d6c-d632-1a2ac1097377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Accuracy: 0.3888888888888889\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "COznRprKuydI",
        "WrEElz43uzEe",
        "Ha2FpwICuyk1",
        "CNcnWqqbuzcH"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}