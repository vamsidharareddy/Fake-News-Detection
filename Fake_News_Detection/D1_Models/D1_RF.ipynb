{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKpr1R9kYA44",
        "outputId": "7b593f97-9ab8-4f63-c165-f9db8cf6da64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive - applicable, if working on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkWoPbCXbxXv"
      },
      "source": [
        "# Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruaMy0qjca92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load the data\n",
        "train_path = '/content/drive/MyDrive/D1/train.csv'\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "test_path = '/content/drive/MyDrive/D1/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Assuming the label column is 'label' and the text column is 'text'\n",
        "X_train = train_data['title']\n",
        "y_train = train_data['label']\n",
        "X_val = val_data['title']\n",
        "y_val = val_data['label']\n",
        "X_test = test_data['title']\n",
        "y_test = test_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "_Sd5ZpnyIy_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize Grid Search CV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Validate the model\n",
        "val_predictions = best_rf.predict(X_val_tfidf)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Test the model\n",
        "test_predictions = best_rf.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2JNQpXYIzCW",
        "outputId": "8d71ee0b-c29b-4008-8fbb-c776962c70a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Validation Accuracy: 0.9324424647364514\n",
            "Test Accuracy: 0.939963614311704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "rf_model_path = '/content/drive/MyDrive/D1/D1_Random/best_rf_model.pkl'\n",
        "joblib.dump(best_rf, rf_model_path)\n",
        "\n",
        "# Save the vectorizer\n",
        "vectorizer_path = '/content/drive/MyDrive/D1/D1_Random/tfidf_vectorizer.pkl'\n",
        "joblib.dump(vectorizer, vectorizer_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfeTPbvWI8yM",
        "outputId": "7c14da37-3fb4-4242-c35f-4be8ba232440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/D1/D1_Random/tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEdrkTpeb-vU"
      },
      "source": [
        "# Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWVWSkPUZhfy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load datasets\n",
        "train_path = '/content/drive/MyDrive/D1/train.csv'\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "test_path = '/content/drive/MyDrive/D1/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Assuming 'label' is the target column\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['label'] = label_encoder.transform(val_data['label'])\n",
        "test_data['label'] = label_encoder.transform(test_data['label'])\n",
        "\n",
        "# Separate features and target\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Initialize the Naive Bayes classifier (GaussianNB)\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Fit the model\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred = nb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy (Naive Bayes): {val_accuracy:.4f}\")\n",
        "\n",
        "# Predict on test set\n",
        "y_test_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate test performance\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy (Naive Bayes): {test_accuracy:.4f}\")\n",
        "\n",
        "# Optionally, print classification report\n",
        "print(\"\\nClassification Report on Test Set (Naive Bayes):\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-eiY6n6cGay"
      },
      "source": [
        "# pbt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrMdC3etZiDm"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn ray tune optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3amt5I2ZiGb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from ray.tune.integration.optuna import OptunaSearch\n",
        "\n",
        "# Load datasets\n",
        "train_path = '/content/drive/MyDrive/D1/train.csv'\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "test_path = '/content/drive/MyDrive/D1/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Preprocess data\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['label'] = label_encoder.transform(val_data['label'])\n",
        "test_data['label'] = label_encoder.transform(test_data['label'])\n",
        "\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Define the objective function for Ray Tune\n",
        "def train_random_forest(config):\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=config[\"n_estimators\"],\n",
        "        max_depth=config[\"max_depth\"],\n",
        "        min_samples_split=config[\"min_samples_split\"],\n",
        "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    tune.report(accuracy=accuracy)\n",
        "\n",
        "# Define search space\n",
        "search_space = {\n",
        "    \"n_estimators\": tune.randint(100, 500),\n",
        "    \"max_depth\": tune.randint(10, 50),\n",
        "    \"min_samples_split\": tune.randint(2, 20),\n",
        "    \"min_samples_leaf\": tune.randint(1, 10)\n",
        "}\n",
        "\n",
        "# Setup Ray Tune reporter\n",
        "reporter = CLIReporter(\n",
        "    metric_columns=[\"accuracy\", \"training_iteration\"]\n",
        ")\n",
        "\n",
        "# Setup Population-Based Training scheduler\n",
        "pbt_scheduler = PopulationBasedTraining(\n",
        "    time_attr=\"training_iteration\",\n",
        "    metric=\"accuracy\",\n",
        "    mode=\"max\",\n",
        "    perturbation_interval=1,\n",
        "    hyperparam_mutations={\n",
        "        \"n_estimators\": [100, 200, 300],\n",
        "        \"max_depth\": [10, 20, 30, 40],\n",
        "        \"min_samples_split\": [2, 5, 10],\n",
        "        \"min_samples_leaf\": [1, 2, 4]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Start the tuning process\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.run(\n",
        "            train_random_forest,\n",
        "            config=search_space,\n",
        "            scheduler=pbt_scheduler,\n",
        "            resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
        "            num_samples=10,\n",
        "            progress_reporter=reporter\n",
        "        ),\n",
        "        resources={\"cpu\": 4, \"gpu\": 0}\n",
        "    ),\n",
        "    tune_config=tune.TuneConfig(\n",
        "        num_samples=10,\n",
        "        scheduler=pbt_scheduler\n",
        "    )\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "# Print the best hyperparameters\n",
        "best_config = results.get_best_result().config\n",
        "print(f\"Best hyperparameters: {best_config}\")\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "best_rf_model = RandomForestClassifier(\n",
        "    n_estimators=best_config[\"n_estimators\"],\n",
        "    max_depth=best_config[\"max_depth\"],\n",
        "    min_samples_split=best_config[\"min_samples_split\"],\n",
        "    min_samples_leaf=best_config[\"min_samples_leaf\"],\n",
        "    random_state=42\n",
        ")\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Optionally, print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlSPW21cKi-"
      },
      "source": [
        "# Genetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41K_kRPKZiyG"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn deap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpHSftjOZi1H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load datasets\n",
        "train_path = '/content/drive/MyDrive/D1/train.csv'\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "test_path = '/content/drive/MyDrive/D1/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Preprocess data\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['label'] = label_encoder.transform(val_data['label'])\n",
        "test_data['label'] = label_encoder.transform(test_data['label'])\n",
        "\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_individual(individual):\n",
        "    # Decode individual (hyperparameters)\n",
        "    n_estimators = individual[0]\n",
        "    max_depth = individual[1]\n",
        "    min_samples_split = individual[2]\n",
        "    min_samples_leaf = individual[3]\n",
        "\n",
        "    # Create and train Random Forest model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "    return accuracy,\n",
        "\n",
        "# Define Genetic Algorithm parameters\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "# Define attributes (hyperparameters)\n",
        "toolbox.register(\"n_estimators\", random.choice, [100, 200, 300])\n",
        "toolbox.register(\"max_depth\", random.choice, [10, 20, 30, 40])\n",
        "toolbox.register(\"min_samples_split\", random.choice, [2, 5, 10])\n",
        "toolbox.register(\"min_samples_leaf\", random.choice, [1, 2, 4])\n",
        "\n",
        "# Define the individual and population size\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.n_estimators, toolbox.max_depth, toolbox.min_samples_split, toolbox.min_samples_leaf), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Define genetic operators\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low=[100, 10, 2, 1], up=[300, 40, 10, 4], indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate_individual)\n",
        "\n",
        "# Number of generations and population size\n",
        "NGEN = 10\n",
        "POP_SIZE = 10\n",
        "\n",
        "def main():\n",
        "    pop = toolbox.population(n=POP_SIZE)\n",
        "\n",
        "    # Evaluate the entire population\n",
        "    fitnesses = list(map(toolbox.evaluate, pop))\n",
        "    for ind, fit in zip(pop, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Begin the evolution\n",
        "    for g in range(NGEN):\n",
        "        print(f\"-- Generation {g+1} --\")\n",
        "\n",
        "        # Select the next generation individuals\n",
        "        offspring = toolbox.select(pop, len(pop))\n",
        "        # Clone the selected individuals\n",
        "        offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "        # Apply crossover and mutation on the offspring\n",
        "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "            if random.random() < 0.5:\n",
        "                toolbox.mate(child1, child2)\n",
        "                del child1.fitness.values\n",
        "                del child2.fitness.values\n",
        "\n",
        "        for mutant in offspring:\n",
        "            if random.random() < 0.2:\n",
        "                toolbox.mutate(mutant)\n",
        "                del mutant.fitness.values\n",
        "\n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "        for ind, fit in zip(invalid_ind, fitnesses):\n",
        "            ind.fitness.values = fit\n",
        "\n",
        "        # Replace the population with the offspring\n",
        "        pop[:] = offspring\n",
        "\n",
        "        # Print the statistics for the current generation\n",
        "        fits = [ind.fitness.values[0] for ind in pop]\n",
        "        print(f\"  Min Fitness: {min(fits)}\")\n",
        "        print(f\"  Max Fitness: {max(fits)}\")\n",
        "\n",
        "    best_ind = tools.selBest(pop, 1)[0]\n",
        "    print(f\"\\nBest individual: {best_ind}\")\n",
        "    print(f\"Best fitness: {best_ind.fitness.values[0]}\")\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    n_estimators, max_depth, min_samples_split, min_samples_leaf = best_ind\n",
        "    best_rf_model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_test_pred = best_rf_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBVLWBFxcQmC"
      },
      "source": [
        "# Hyper band"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb2hT_AAZjqM"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn ray[tune]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLt7fbJ4ZjtN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import HyperBandForBOHB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load datasets\n",
        "train_path = '/content/drive/MyDrive/D1/train.csv'\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "test_path = '/content/drive/MyDrive/D1/test.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "\n",
        "# Preprocess data\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['label'] = label_encoder.fit_transform(train_data['label'])\n",
        "val_data['label'] = label_encoder.transform(val_data['label'])\n",
        "test_data['label'] = label_encoder.transform(test_data['label'])\n",
        "\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_val = val_data.drop(columns=['label'])\n",
        "y_val = val_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Define the trainable function\n",
        "def train_random_forest(config):\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=config[\"n_estimators\"],\n",
        "        max_depth=config[\"max_depth\"],\n",
        "        min_samples_split=config[\"min_samples_split\"],\n",
        "        min_samples_leaf=config[\"min_samples_leaf\"],\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Define search space\n",
        "search_space = {\n",
        "    \"n_estimators\": tune.randint(100, 500),\n",
        "    \"max_depth\": tune.randint(10, 50),\n",
        "    \"min_samples_split\": tune.randint(2, 20),\n",
        "    \"min_samples_leaf\": tune.randint(1, 10)\n",
        "}\n",
        "\n",
        "# Setup Ray Tune reporter\n",
        "reporter = CLIReporter(\n",
        "    metric_columns=[\"accuracy\", \"training_iteration\"]\n",
        ")\n",
        "\n",
        "# Setup HyperBand scheduler for BOHB (Bayesian Optimization HyperBand)\n",
        "hyperband_scheduler = HyperBandForBOHB(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=100,\n",
        "    reduction_factor=3,\n",
        "    metric=\"accuracy\",\n",
        "    mode=\"max\"\n",
        ")\n",
        "\n",
        "# Start the tuning process\n",
        "analysis = tune.run(\n",
        "    train_random_forest,\n",
        "    config=search_space,\n",
        "    scheduler=hyperband_scheduler,\n",
        "    resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
        "    num_samples=20,\n",
        "    progress_reporter=reporter,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Get the best trial\n",
        "best_trial = analysis.get_best_trial(metric=\"accuracy\", mode=\"max\")\n",
        "best_config = best_trial.config\n",
        "best_accuracy = best_trial.last_result[\"accuracy\"]\n",
        "\n",
        "print(f\"Best hyperparameters: {best_config}\")\n",
        "print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "best_rf_model = RandomForestClassifier(\n",
        "    n_estimators=best_config[\"n_estimators\"],\n",
        "    max_depth=best_config[\"max_depth\"],\n",
        "    min_samples_split=best_config[\"min_samples_split\"],\n",
        "    min_samples_leaf=best_config[\"min_samples_leaf\"],\n",
        "    random_state=42\n",
        ")\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred = best_rf_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT + GSCV"
      ],
      "metadata": {
        "id": "27uQJ9uLI-mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "bert_model.eval()\n",
        "\n",
        "# Tokenizer for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the EnsembleModel class\n",
        "class EnsembleModel:\n",
        "    def __init__(self, bert_model, rf_model, tokenizer, vectorizer, batch_size=32):\n",
        "        self.bert_model = bert_model\n",
        "        self.rf_model = rf_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vectorizer = vectorizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bert_preds = []\n",
        "        rf_preds = []\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch_texts = texts[i:i + self.batch_size]\n",
        "\n",
        "            # Get BERT predictions\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                bert_output = outputs.logits.cpu().numpy()\n",
        "                bert_preds.append(bert_output)\n",
        "\n",
        "            # Get Random Forest predictions\n",
        "            X_batch = self.vectorizer.transform(batch_texts)\n",
        "            rf_output = self.rf_model.predict_proba(X_batch)[:, 1]\n",
        "            rf_preds.append(rf_output)\n",
        "\n",
        "        bert_preds = np.concatenate(bert_preds, axis=0)\n",
        "        rf_preds = np.concatenate(rf_preds, axis=0)\n",
        "\n",
        "        # Combine predictions (simple averaging)\n",
        "        combined_outputs = (bert_preds[:, 1] + rf_preds) / 2\n",
        "        return (combined_outputs > 0.5).astype(int)\n",
        "\n",
        "# Load the Random Forest model and TF-IDF vectorizer\n",
        "rf_model_path = '/content/drive/MyDrive/D1/D1_Random/best_rf_model.pkl'\n",
        "vectorizer_path = '/content/drive/MyDrive/D1/D1_Random/tfidf_vectorizer.pkl'\n",
        "\n",
        "best_rf_model = joblib.load(rf_model_path)\n",
        "vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/D1/val.csv')\n",
        "test_texts = test_data['title'].tolist()\n",
        "\n",
        "# Instantiate the ensemble model\n",
        "ensemble_model = EnsembleModel(bert_model, best_rf_model, tokenizer, vectorizer, batch_size=32)\n",
        "\n",
        "# Make predictions on the test set\n",
        "ensemble_predictions = ensemble_model.predict(test_texts)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_data['label'], ensemble_predictions)\n",
        "print(f\"Ensemble Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb0LuoE7Wepd",
        "outputId": "37a9c06d-eb80-49b8-c29c-9285452ad8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Test Accuracy: 0.5115070527097253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with sampling"
      ],
      "metadata": {
        "id": "y01MpsKLJCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "bert_model.eval()\n",
        "\n",
        "# Tokenizer for BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the EnsembleModel class\n",
        "class EnsembleModel:\n",
        "    def __init__(self, bert_model, rf_model, tokenizer, vectorizer, batch_size=32, subset_size=None):\n",
        "        self.bert_model = bert_model\n",
        "        self.rf_model = rf_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vectorizer = vectorizer\n",
        "        self.batch_size = batch_size\n",
        "        self.subset_size = subset_size\n",
        "\n",
        "    def predict(self, texts):\n",
        "        # Determine subset size\n",
        "        if self.subset_size is None:\n",
        "            self.subset_size = len(texts)  # Use entire dataset if subset size is not provided\n",
        "        else:\n",
        "            self.subset_size = min(self.subset_size, len(texts))  # Ensure subset size is within dataset length\n",
        "\n",
        "        texts = texts[:self.subset_size]\n",
        "\n",
        "        bert_preds = []\n",
        "        rf_preds = []\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch_texts = texts[i:i + self.batch_size]\n",
        "\n",
        "            # Get BERT predictions\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                bert_output = outputs.logits.cpu().numpy()\n",
        "                bert_preds.append(bert_output)\n",
        "\n",
        "            # Get Random Forest predictions\n",
        "            X_batch = self.vectorizer.transform(batch_texts)\n",
        "            rf_output = self.rf_model.predict_proba(X_batch)[:, 1]\n",
        "            rf_preds.append(rf_output)\n",
        "\n",
        "        bert_preds = np.concatenate(bert_preds, axis=0)\n",
        "        rf_preds = np.concatenate(rf_preds, axis=0)\n",
        "\n",
        "        # Combine predictions (simple averaging)\n",
        "        combined_outputs = (bert_preds[:, 1] + rf_preds) / 2\n",
        "        return (combined_outputs > 0.5).astype(int)\n",
        "\n",
        "# Load the Random Forest model and TF-IDF vectorizer\n",
        "rf_model_path = '/content/drive/MyDrive/D1/D1_Random/best_rf_model.pkl'\n",
        "vectorizer_path = '/content/drive/MyDrive/D1/D1_Random/tfidf_vectorizer.pkl'\n",
        "\n",
        "best_rf_model = joblib.load(rf_model_path)\n",
        "vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "# Load validation data\n",
        "val_path = '/content/drive/MyDrive/D1/val.csv'\n",
        "val_data = pd.read_csv(val_path)\n",
        "y_val = val_data['label'].values\n",
        "\n",
        "# Instantiate the ensemble model with subset size\n",
        "ensemble_model = EnsembleModel(bert_model, best_rf_model, tokenizer, vectorizer, batch_size=32, subset_size=1000)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_titles = val_data['title'].tolist()\n",
        "ensemble_predictions = ensemble_model.predict(val_titles)\n",
        "\n",
        "# Calculate ensemble accuracy\n",
        "ensemble_accuracy = accuracy_score(y_val[:ensemble_model.subset_size], ensemble_predictions)\n",
        "print(f\"Ensemble Validation Accuracy: {ensemble_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Essbf6DPJID-",
        "outputId": "dd74d05d-a213-4a87-89d2-abedbb039d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Validation Accuracy: 0.723\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "IkWoPbCXbxXv",
        "BEdrkTpeb-vU",
        "s-eiY6n6cGay",
        "pYlSPW21cKi-",
        "EBVLWBFxcQmC"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}